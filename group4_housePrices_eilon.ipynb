{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Data Analytics and Prediction\n",
    "\n",
    "## Group no. 4 - Housing Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew \n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for development stages\n",
    "DEBUG_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eilon - used for development stage - I need to set my working directory manually\n",
    "EILON_SET_WORK_DIR = True\n",
    "if (EILON_SET_WORK_DIR):\n",
    "# Now change the directory\n",
    "    import os\n",
    "    os.chdir(\"C:/Users/bareilon/Documents/Personal/MBA/IDC BIG DATA/Mini Semester 1/Business Data Analytics/exercises/IDC-BDA-Exercises\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Business objectives and targets\n",
    "\n",
    "#### Business objective: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read, explore and prepare data\n",
    "\n",
    "#### 2.1. Download and read the data\n",
    "\n",
    "The dataset is taken from a Kaggle competition: <br>\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "Read the csv file to a data frame. Note that the data is already split to train and test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Taking a quick look at the data \n",
    "Using the functions: \n",
    "* head\n",
    "* info \n",
    "* describe\n",
    "\n",
    "\n",
    "1. The prediction target (y) is the SalePrice column.\n",
    "2. We will handle some missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"Data/train.csv\")\n",
    "train_len = df_train.shape[0]\n",
    "df_test = pd.read_csv(\"Data/test.csv\")\n",
    "df_test[\"SalePrice\"] = 0 \n",
    "df_merged = df_train.append(df_test)\n",
    "#df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = df_train[df_train.select_dtypes(exclude='object').columns]\n",
    "corr_mat = corr_mat.drop('Id', axis=1)\n",
    "corr_mat = corr_mat.corr()\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(corr_mat, vmax=.8, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Correlations\n",
    "n = 10 #number of variables for heatmap\n",
    "cols = corr_mat.nlargest(n, 'SalePrice')['SalePrice'].index\n",
    "cm = np.corrcoef(df_train[cols].values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': n}, yticklabels=cols.values, xticklabels=cols.values, vmax=.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all columns with missing values\n",
    "def get_null_list():\n",
    "    _null_list = df_merged.isna().sum()\n",
    "    _null_list = _null_list[_null_list > 0].sort_values(ascending=False)\n",
    "    return _null_list\n",
    "\n",
    "null_list = get_null_list()\n",
    "null_list.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: PoolQC\n",
    "If the Pool Area is 0, then the Pool QC is 'NA', as described in the Data Description file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (df_merged.PoolArea == 0) & (df_merged.PoolQC.isna() == True)\n",
    "df_merged.loc[idx, 'PoolQC'] = df_merged.loc[idx, 'PoolQC'].fillna('NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But we still have 3 outliers. Houses with pools but the qaulity is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[(df_merged.PoolQC.isna() == True) & (df_merged.PoolArea > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.PoolQC.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data of the houses wit the pools but no quality grading.\n",
    "We'll review these features: Overall Condition, External Condition, Garage Condition, Heating Quality etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_cols = ['OverallCond','ExterCond','ExterQual','GarageCond','GarageQual','HeatingQC','FireplaceQu','KitchenQual','BsmtFinType1','BsmtFinType2','Fence','PoolQC']\n",
    "df_merged[qc_cols][(df_merged.PoolQC.isna() == True) & (df_merged.PoolArea > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume the quality is Typical/Average (TA), basd on the other parameters. <br>\n",
    "If there was a lot of data about houses with pools, we could build a model to predict this variable based on other quality fators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (df_merged.PoolQC.isna() == True) & (df_merged.PoolArea > 0)\n",
    "df_merged.loc[idx, 'PoolQC'] = df_merged.loc[idx, 'PoolQC'].fillna('TA')\n",
    "df_merged.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.PoolQC.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.PoolQC.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'MiscFeature' Column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's overview the 'MiscFeature' column\n",
    "According to the data description:\n",
    "MiscFeature: Miscellaneous feature not covered in other categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.MiscFeature.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fill in all the null values with 'NA'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df_merged.MiscFeature.isna()\n",
    "df_merged.loc[idx, 'MiscFeature'] = df_merged.loc[idx, 'MiscFeature'].fillna('NA')\n",
    "df_merged[idx].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait, let's check of there is a house with a Miscellaneous Value greater then 0 that we wrongly labeled 'MiscFeature' as 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (df_merged.MiscFeature == 'NA') & (df_merged.MiscVal > 0)\n",
    "df_merged[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one house that we missslabeled. Without other info, we will set it to \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.loc[idx, 'MiscFeature'] = 'Othr'\n",
    "df_merged[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.MiscFeature.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.MiscFeature.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fence Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.Fence.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df_merged.Fence.isna()\n",
    "df_merged.loc[idx, 'Fence'] = df_merged.loc[idx, 'Fence'].fillna('NA')\n",
    "df_merged[idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.Fence.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alley Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.Alley.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df_merged.Alley.isna()\n",
    "df_merged.loc[idx, 'Alley'] = df_merged.loc[idx, 'Alley'].fillna('NA')\n",
    "df_merged[idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.Alley.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'FireplaceQu' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.FireplaceQu.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'FireplaceQu'\n",
    "idx = df_merged[col].isna()\n",
    "df_merged.loc[idx, col] = df_merged.loc[idx, col].fillna('NA')\n",
    "df_merged[idx].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.FireplaceQu.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if we miss labeled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[(df_merged.Fireplaces > 0) & (df_merged.FireplaceQu == 'NA')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we're good..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garage relaed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the columns related to Garages \n",
    "cols = df_merged.filter(regex='Garage').columns\n",
    "cols = cols.values\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find indexes of homes without garages\n",
    "idx = df_merged[(df_merged.GarageType.isna()) & (df_merged.GarageYrBlt.isna()) & (df_merged.GarageFinish.isna()) & \\\n",
    "                (df_merged.GarageCars == 0)   & (df_merged.GarageArea == 0)    & (df_merged.GarageQual.isna())   & \\\n",
    "                (df_merged.GarageCond.isna()) ].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values\n",
    "df_merged.loc[idx, 'GarageType'] = df_merged.loc[idx, 'GarageType'].fillna('NA')\n",
    "df_merged.loc[idx, 'GarageFinish'] = df_merged.loc[idx, 'GarageFinish'].fillna('NA')\n",
    "df_merged.loc[idx, 'GarageQual'] = df_merged.loc[idx, 'GarageQual'].fillna('NA')\n",
    "df_merged.loc[idx, 'GarageCond'] = df_merged.loc[idx, 'GarageCond'].fillna('NA')\n",
    "df_merged.loc[idx, 'GarageYrBlt'] = df_merged.loc[idx, 'GarageYrBlt'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_list = df_merged[cols].isna().sum()\n",
    "null_list[null_list > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have a few rows with missing values ralted to Garages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = ((df_merged.GarageCond.isna()) | (df_merged.GarageYrBlt.isna()) | (df_merged.GarageFinish.isna()) | \\\n",
    "                (df_merged.GarageQual.isna()) | (df_merged.GarageCond.isna()) | (df_merged.GarageYrBlt.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.loc[idx, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the year the house was built\n",
    "df_merged.loc[idx, 'GarageYrBlt'] = df_merged.loc[idx, 'YearBuilt']\n",
    "df_merged.loc[idx,cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill in the 'Finish' based on the most common value for 'Detached' Garages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.GarageFinish[df_merged.GarageType == 'Detchd'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most 'Detached' Garages are 'Unf' (Unfinished), we will set the value to 'Unf'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.loc[idx, 'GarageFinish'] = 'Unf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.GarageArea[df_merged.GarageType == 'Detchd'].count()\n",
    "df_merged.loc[idx,cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same for Garage Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.GarageQual[df_merged.GarageType == 'Detchd'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the most common quality for Detached Garages which is 'TA'\n",
    "df_merged.loc[idx, 'GarageQual'] = 'TA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again for Garage Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the most common Condition for Detached Garages which is 'TA'\n",
    "df_merged.loc[idx, 'GarageCond'] = 'TA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.GarageCond[df_merged.GarageType == 'Detchd'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to handle row id 1116.\n",
    "Let's try to estimate the size of the Garage, based on the year the house was built.\n",
    "We assume garages back then had some stadard size, and most families owned one car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_min = 1920\n",
    "_max = 1925\n",
    "df_merged[(df_merged.GarageYrBlt > _min) & (df_merged.GarageYrBlt < _max)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['GarageCars'][(df_merged.GarageYrBlt > _min) & (df_merged.GarageYrBlt < _max)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1116\n",
    "garage_mean_area = df_merged['GarageArea'][(df_merged.GarageYrBlt > _min) & (df_merged.GarageYrBlt < _max) & (df_merged.GarageCars == 1)].mean()\n",
    "df_merged.loc[idx, 'GarageCars'] = 1.0\n",
    "df_merged.loc[idx, 'GarageArea'] = round(garage_mean_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LotFrontage\n",
    "It seems this column is tricky. <br>\n",
    "Let's examine the correlation between the 'SalePrice' and the 'LotFrontage' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df_merged[:train_len].copy()\n",
    "df_corr['SalePrice'] = df.SalePrice.astype(int)\n",
    "df_corr = df_corr[df_corr.LotFrontage > 0] \n",
    "df_corr = df_corr.loc[: ,['LotFrontage','SalePrice']]\n",
    "df_corr.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a 35% correlation which is not too high, nor too low.\n",
    "For now, we will fill in the Mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n",
    "df_merged[\"LotFrontage\"] = df_merged.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_merged.LotFrontage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basement Realted Columns\n",
    "BsmtExposure    82 <br>\n",
    "BsmtCond        82 <br>\n",
    "BsmtQual        81 <br>\n",
    "BsmtFinType2    80 <br>\n",
    "BsmtFinType1    79 <br>\n",
    "\n",
    "and we also have other related columns: <br>\n",
    "TotalBsmtSF <br>\n",
    "BsmtUnfSF <br>\n",
    "BsmtFullBath <br>\n",
    "BsmtHalfBath <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_merged.filter(regex='Bsmt').columns\n",
    "cols = cols.values\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in 0 if the TotalBsmtSF is null\n",
    "idx = df_merged.TotalBsmtSF.isna()\n",
    "df_merged.loc[idx, 'TotalBsmtSF'] = df_merged.loc[idx, 'TotalBsmtSF'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indexes of rows with missing Basement parameters\n",
    "idx = ((df_merged.BsmtQual.isna()) & (df_merged.BsmtCond.isna()) & (df_merged.BsmtExposure.isna()) & \\\n",
    "                (df_merged.BsmtFinType1.isna()) & (df_merged.BsmtFinType2.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.loc[idx, cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (df_merged.TotalBsmtSF == 0.0)\n",
    "str_cols = ['BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2']\n",
    "df_merged.loc[idx, str_cols] = df_merged.loc[idx, str_cols].apply(lambda x: x.fillna('NA'), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = ((df_merged.TotalBsmtSF == 0) & ((df_merged.BsmtUnfSF.isna()) | (df_merged.BsmtFullBath.isna()) | \\\n",
    "                (df_merged.BsmtHalfBath.isna())))\n",
    "num_cols = ['BsmtFinSF1','BsmtFinSF2','BsmtHalfBath','BsmtFullBath', 'BsmtUnfSF']\n",
    "df_merged.loc[idx, num_cols] = df_merged.loc[idx, num_cols].apply(lambda x: x.fillna(0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.loc[:,cols][df_merged.loc[:,cols].isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.BsmtQual.value_counts()\n",
    "df_merged[df_merged.BsmtQual.isna()] = df_merged[df_merged.BsmtQual.isna()].fillna('TA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.BsmtCond.value_counts()\n",
    "df_merged[df_merged.BsmtCond.isna()] = df_merged[df_merged.BsmtCond.isna()].fillna('TA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.BsmtExposure.value_counts()\n",
    "df_merged[df_merged.BsmtExposure.isna()] = df_merged[df_merged.BsmtExposure.isna()].fillna('No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.BsmtFinType2.value_counts()\n",
    "df_merged[df_merged.BsmtFinType2.isna()] = df_merged[df_merged.BsmtFinType2.isna()].fillna('Unf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_cols = ['MasVnrType', 'MasVnrArea']\n",
    "df_merged[mas_cols][df_merged[mas_cols].isnull().any(axis=1)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.MasVnrType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = ( (df_merged.MasVnrType.isna()) & (df_merged.MasVnrArea > 0.0) )\n",
    "df_merged.loc[idx, 'MasVnrType'] = df_merged.loc[idx, 'MasVnrType'].fillna('BrkFace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = ( (df_merged.MasVnrType.isna()) & (df_merged.MasVnrArea.isna()) )\n",
    "df_merged.loc[idx, 'MasVnrType'] = df_merged.loc[idx, 'MasVnrType'].fillna('None')\n",
    "df_merged.loc[idx, 'MasVnrArea'] = df_merged.loc[idx, 'MasVnrArea'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check what is still missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_list = get_null_list()\n",
    "null_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSZoning Column <br>\n",
    "4 missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.MSZoning.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'RL' is the most common value so we can assume the missing values are RL.\n",
    "We can also try later to reun the model with nulls and see the differance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df_merged.MSZoning.isna()\n",
    "df_merged.loc[idx, 'MSZoning'] = df_merged.loc[idx, 'MSZoning'].fillna('RL')\n",
    "df_merged.loc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.Functional.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typ (Typical) is the most common value.\n",
    "The data description file tells us we can assume Typical unless specified differently:\n",
    "Functional: Home functionality (Assume typical unless deductions are warranted)\n",
    "Hence, we will fill in Typ for the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[df_merged.Functional.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Functional'\n",
    "df_merged[col] = df_merged[col].fillna('Typ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_list = get_null_list()\n",
    "null_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Utilities'\n",
    "df_merged[df_merged[col].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'AllPub' is the most common value (99.99 percent)\n",
    "All records are \"AllPub\", except for one \"NoSeWa\" and 2 missing values. Since the house with 'NoSewa' is in the test set (no price info), this feature won't help in predicting the price. We will drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.drop(col,  axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns: SaleType, KitchenQual, Electrical, Exterior2nd, Exterior1st\n",
    "have 1 or 2 missing values so we will fill in with the most common value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cols = ['SaleType', 'KitchenQual', 'Electrical', 'Exterior2nd', 'Exterior1st']\n",
    "for col in null_cols:\n",
    "    df_merged[col] = df_merged[col].fillna(df_merged[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_list = get_null_list()\n",
    "null_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "# Sale Price and Lot Area pairplot\n",
    "sns.distplot(df_merged[:train_len-100].LotArea)\n",
    "ax = sns.scatterplot(x=\"LotArea\", y=\"SalePrice\", data=df_merged[:train_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged.LotArea[df_merged.LotArea > 100000] = (df_merged.LotArea[df_merged.LotArea > 100000] / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=\"GrLivArea\", y=\"SalePrice\", data=df_merged[:train_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Nominal variables into dummies\n",
    "df_merged = pd.get_dummies(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing MSSubClass into a categorical variable\n",
    "df_merged['MSSubClass'] = df_merged['MSSubClass'].astype(str)\n",
    "\n",
    "# Changing OverallCond into a categorical variable\n",
    "df_merged['OverallCond'] = df_merged['OverallCond'].astype(str)\n",
    "\n",
    "# Year and month sold are transformed into categorical features.\n",
    "df_merged['YrSold'] = df_merged['YrSold'].astype(str)\n",
    "df_merged['MoSold'] = df_merged['MoSold'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Total Living Area column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['GrLivArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'BsmtUnfSF','LowQualFinSF', 'TotLivArea']\n",
    "df_merged['TotLivArea'] = df_merged['1stFlrSF'] + df_merged['2ndFlrSF'] + df_merged['TotalBsmtSF']\n",
    "df_merged[cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Exploratory data analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exploration is required to understand what is your data, and which preparations are required on it. A common visualization is a histogram. Use histogram to understand the values distribution, because many statistical model assume normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not DEBUG_MODE):\n",
    "    df_merged.hist(bins = 25, figsize = (20,15)) #Check the hist parameters by clicking on the Tab completion. \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not DEBUG_MODE):\n",
    "    df_merged.LotArea[df_merged['LotArea'] < 5000].hist(bins = 50, figsize = (10,5)) #Check the hist parameters by clicking on the Tab completion. \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not DEBUG_MODE):\n",
    "    display(df_merged.isnull().sum(),df_merged.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Data manipulations\n",
    "\n",
    "After we looked at the data we can prepare it to analysis. \n",
    "\n",
    "From understanding the histograms, and talking with experts we might decide to drop or change columns, or to split the data by rows. \n",
    "\n",
    "For example: \n",
    "\n",
    "...\n",
    "\n",
    "An option that we will take here to handle this is:\n",
    "1. Take out the few rows and put them aside for separate handling\n",
    "2. Drop this predictor from data\n",
    "\n",
    "(Note that after the data manipulation you can rerun the describe or histogram above to see changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically data scientists continue back and forth between diffrent data visualizations and manipulations, but for this exercise we will stop here. We will dive more to this on next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Categorical values\n",
    "\n",
    "Data manipulations require also handling of categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Transform the data to matrix of X and y, spliting to Train and Test\n",
    "\n",
    "Let's now prepae to modeling:\n",
    "1. Split between X the predictors and y the target\n",
    "2. Turn from data frame to matrix\n",
    "3. Split X and y to train data set and test data set, with matching indexes between X and y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_test_split is the first function we are using from sklearn.\n",
    "Learn more about it function at: <br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html \n",
    "\n",
    "\n",
    "sklearn handles numpy arrays, whereas until now we handled a dataframe.\n",
    "Lets check that indeed we changed the type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable\n",
    "\n",
    "SalePrice is the variable we decided to predict. So let's analyze this variable.\n",
    "We plotted a graph of salePrice. Since target distribution is not normalized, we performed log-transformation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df_merged[:train_len].copy()\n",
    "df_corr['SalePrice'] = df.SalePrice.astype(int)\n",
    "sns.distplot(df_corr['SalePrice'], fit=norm)\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df_corr['SalePrice'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_corr['SalePrice'], plot=plt)\n",
    "plt.show()\n",
    "\n",
    "df_corr['SalePriceLog'] = np.log1p(df_corr['SalePrice'])\n",
    "#Check the new distribution \n",
    "sns.distplot(df_corr['SalePriceLog'] , fit=norm);\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(df_corr['SalePriceLog'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(df_corr['SalePriceLog'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def calc_rmse(y_actual, y_predicted):\n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged[:train_len].copy()\n",
    "df['SalePriceLog'] = np.log1p(df['SalePrice'])\n",
    "df = df.drop(['Id','SalePrice'],axis=1)\n",
    "#split the data into train and test sections\n",
    "train_df = df\n",
    "#train_df, test_df= train_test_split (df, test_size = 0.1, random_state=7) \n",
    "# train_test_split is from sklearn. It requiers that the data will be a numpy array, that is numbers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Train Data\n",
    "X_train = train_df.drop('SalePriceLog',axis=1).values\n",
    "y_train = train_df['SalePriceLog'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fit Models\n",
    "\n",
    "We will start with the basic model of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Fit the selected model\n",
    "Training the model, using sklearn, is typically only one command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model evaluation\n",
    "\n",
    "Evaluating the model can also be done in one command.\n",
    "\n",
    "We can evalute the model that was trained on train_x by its prediction of test_x compared to test_y in one command. On next lessons we will learn more evaluation methods, as usually decision is taken by combined evaluation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg_pred = reg.predict(X_train)\n",
    "calc_rmse(y_train, y_reg_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the evaluation on test was very good. Yet, model score on train is much higher. This may indicate that we might be in an overfit to the train data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Model\n",
    "### Gradient Boosting Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1,\n",
    "                                   max_depth=8, max_features='sqrt',\n",
    "                                   min_samples_leaf=10, min_samples_split=4, \n",
    "                                   loss='huber', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gbr_pred = gbr.predict(X_train)\n",
    "calc_rmse(y_train, y_gbr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000,12 -> 0.05486181834047261   100,20 -> 0.0.05248432308239742"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rnf = RandomForestRegressor(n_estimators=100, max_depth=20, n_jobs=-1)\n",
    "rnf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rnf_pred = rnf.predict(X_train)\n",
    "calc_rmse(y_train, y_rnf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Stacked Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_average = 0.5 * y_gbr_pred + 0.3 * y_rnf_pred +  + 0.2 * y_reg_pred\n",
    "calc_rmse(y_train, y_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = rnf.predict(X_test)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.scatter(y_test, y_pred, label = 'Predict')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predict / Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on test data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged[train_len:].copy()\n",
    "X_pred = df.drop(['Id','SalePrice'],axis=1).values\n",
    "y_pred = np.exp(0.5 * gbr.predict(X_pred) + 0.3 * rnf.predict(X_pred) +  0.2 * reg.predict(X_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df = pd.DataFrame()\n",
    "kaggle_df['Id'] = df_merged.Id[train_len:]\n",
    "kaggle_df['SalePrice'] = y_pred\n",
    "kaggle_df.to_csv('Prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Communicate\n",
    "Now that you have a good working model you need to communicate your results.\n",
    "\n",
    "If this is a predict project, you may decide not to communicate details externaly, only your evaluation results.\n",
    "\n",
    "You need to communicate how you got to the results, to customers on infer project, and internaly on predict project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_merged.iloc[:train_len].copy()\n",
    "df_hb = tmp[tmp.HalfBath > 0].copy()\n",
    "df_hb['HalfBath'] = df_hb['HalfBath'] - 1 \n",
    "df_hb['FullBath'] = df_hb['FullBath'] + 1 \n",
    "X = df_hb.drop(['Id','SalePrice'],axis=1).values\n",
    "y_hb = np.exp(gbr.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the price differance and plot the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff =  y_hb - df_hb['SalePrice']\n",
    "\n",
    "limit = 30000\n",
    "plt.xlim(-limit, limit)\n",
    "plt.axvline(10000, color='coral')\n",
    "sns.distplot(diff ,color=\"teal\", axlabel=\"Predicted Price Change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Score (GBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_score = gbr.feature_importances_\n",
    "feature_names = df_merged.drop(['Id','SalePrice'], axis=1).columns\n",
    "df_feature_score = pd.DataFrame(data=feature_names, columns=[\"Feature\"])\n",
    "df_feature_score[\"score\"]= feature_score\n",
    "df_feature_score = df_feature_score.sort_values(by=['score'], ascending=False)\n",
    "df_feature_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "sns.barplot(y=\"Feature\", x=\"score\", data=df_feature_score.head(26))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search optimization for Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_n_estimators_range = [10, 100, 500]\n",
    "param_max_depth_range =  [7, 12, 15, 20] \n",
    "\n",
    "param_grid = [{'n_estimators': param_n_estimators_range, 'max_depth':param_max_depth_range},\n",
    "              {'n_estimators': param_n_estimators_range, 'max_depth':param_max_depth_range, 'bootstrap': ['False']}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV (estimator=RandomForestRegressor(), param_grid = param_grid, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs = gs.fit (X=X_train, y=y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The best score is:', gs.best_score_, '\\nThe best parameters are:', gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search optimization for GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_n_estimators_range = [1000, 3000]\n",
    "param_max_depth_range =  [4, 8] \n",
    "param_learning_rate = [0.05, 0.1]\n",
    "\n",
    "param_grid = [{'n_estimators': param_n_estimators_range, 'max_depth':param_max_depth_range, 'learning_rate':param_learning_rate}]\n",
    "gs = GridSearchCV (estimator=GradientBoostingRegressor(), param_grid = param_grid, cv=10)\n",
    "gs = gs.fit (X=X_train, y=y_train.ravel())\n",
    "print ('The best score is:', gs.best_score_, '\\nThe best parameters are:', gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
